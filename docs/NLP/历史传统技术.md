# å†å²ä¼ ç»ŸæŠ€æœ¯

> ç”±äºæ–°æŠ€æœ¯çš„å‡ºç°ï¼Œæœ‰äº›æŠ€æœ¯å·²å¾ˆå°‘ä½¿ç”¨ã€‚

## N-Grams

In natural language, precise meaning of words can only be determined in context. For example, meanings of *neural network* and *fishing network* are completely different. One of the ways to take this into account is to build our model on pairs of words, and considering word pairs as separate vocabulary tokens. In this way, the sentence *I like to go fishing* will be represented by the following sequence of tokens: *I like*, *like to*, *to go*, *go fishing*. The problem with this approach is that the dictionary size grows significantly, and combinations like *go fishing* and *go shopping* are presented by different tokens, which do not share any semantic similarity despite the same verb.

In some cases, we may consider using tri-grams -- combinations of three words -- as well. Thus the approach is such is often called **n-grams**. Also, it makes sense to use n-grams with character-level representation, in which case n-grams will roughly correspond to different syllabi.

## Static Word Embedding

æ˜¯æŒ‡æ¯ä¸ªè¯è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå›ºå®šçš„å‘é‡ï¼ˆé€šå¸¸æ˜¯å®æ•°å‘é‡ï¼‰ï¼Œè¿™ä¸ªå‘é‡åœ¨ä¸Šä¸‹æ–‡ä¸­ä¸å‘ç”Ÿæ”¹å˜ã€‚

åŒä¸€ä¸ªè¯åœ¨ä¸åŒå¥å­ä¸­å…·æœ‰**ç›¸åŒçš„å‘é‡è¡¨ç¤º**ã€‚

**ä¸èƒ½å¤„ç†å¤šä¹‰è¯ï¼ˆpolysemyï¼‰**ï¼š

- "bank"ï¼ˆæ²³å²¸ vs é“¶è¡Œï¼‰åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ä»æ˜¯åŒä¸€ä¸ªå‘é‡ã€‚

å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯**ä¸æ•æ„Ÿ**ã€‚

åœ¨ç°ä»£ NLP ä¸­ï¼Œå·²è¢« **ä¸Šä¸‹æ–‡è¯åµŒå…¥ï¼ˆContextual Embeddingï¼‰**ï¼ˆå¦‚ BERTï¼‰å¹¿æ³›å–ä»£ã€‚

### ğŸ”¹ å¸¸è§çš„é™æ€è¯åµŒå…¥æ¨¡å‹

| æ¨¡å‹åç§°     | ç®€ä»‹                                               | è®­ç»ƒæ–¹æ³•                   |
| ------------ | -------------------------------------------------- | -------------------------- |
| **Word2Vec** | ç”± Google æå‡ºï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡é¢„æµ‹ç›®æ ‡è¯æˆ–åä¹‹         | Skip-gram / CBOW           |
| **GloVe**    | Global Vectorsï¼Œç”± Stanford æå‡ºï¼Œç»“åˆå…¨å±€å…±ç°çŸ©é˜µ | åŸºäºå…±ç°ç»Ÿè®¡çš„çŸ©é˜µå› å¼åˆ†è§£ |
| **FastText** | Facebook æå‡ºï¼Œå¯ä»¥å¤„ç†æœªç™»å½•è¯ï¼ˆOOVï¼‰             |                            |

## Bag-of-Words and TF/IDF

When solving tasks like text classification, we need to be able to represent text by one fixed-size vector, which we will use as an input to final dense classifier. One of the simplest ways to do that is to combine all individual word representations, eg. by adding them. If we add one-hot encodings of each word, we will end up with a vector of frequencies, showing how many times each word appears inside the text. Such representation of text is called **bag of words** (BoW).

A BoW essentially represents which words appear in text and in which quantities, which can indeed be a good indication of what the text is about. For example, news article on politics is likely to contains words such as *president* and *country*, while scientific publication would have something like *collider*, *discovered*, etc. Thus, word frequencies can in many cases be a good indicator of text content.

The problem with BoW is that certain common words, such as *and*, *is*, etc. appear in most of the texts, and they have highest frequencies, masking out the words that are really important. We may lower the importance of those words by taking into account the frequency at which words occur in the whole document collection. This is the main idea behind TF/IDF approach.

However, none of those approaches can fully take into account the **semantics** of text. We need more powerful neural networks models to do this.
